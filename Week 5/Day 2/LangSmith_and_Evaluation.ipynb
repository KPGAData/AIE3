{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa_QpI0RXQKx"
      },
      "source": [
        "# LangSmith and Evaluation Overview with AI Makerspace\n",
        "\n",
        "Today we'll be looking at an amazing tool:\n",
        "\n",
        "[LangSmith](https://docs.smith.langchain.com/)!\n",
        "\n",
        "This tool will help us monitor, test, debug, and evaluate our LangChain applications - and more!\n",
        "\n",
        "We'll also be looking at a few Advanced Retrieval techniques along the way - and evaluate it using LangSmith!\n",
        "\n",
        "âœ‹BREAKOUT ROOM #1:\n",
        "- Task 1: Dependencies and OpenAI API Key\n",
        "- Task 2: Basic RAG Chain\n",
        "- Task 3: Setting Up LangSmith\n",
        "- Task 4: Examining the Trace in LangSmith!\n",
        "- Task 5: Create Testing Dataset\n",
        "\n",
        "âœ‹BREAKOUT ROOM #2:\n",
        "- Task 1: Parent Document Retriever\n",
        "- Task 2: Ensemble Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw5ok9p-XuUs"
      },
      "source": [
        "## Task 1: Dependencies and OpenAI API Key\n",
        "\n",
        "We'll be using OpenAI's suite of models today to help us generate and embed our documents for a simple RAG system built on top of LangChain's blogs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhSjB1O6-Y0J",
        "outputId": "9cc0c072-1117-4863-8010-ee37e8e33a3d"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_core langchain_openai langchain_community langchain-qdrant qdrant-client langsmith openai tiktoken cohere -qU\n",
        "!pip install -qU lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADl8-whIAUHD",
        "outputId": "a5794372-be42-46ee-cf7d-4e5628e97e9a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_NpPwk1YAgl"
      },
      "source": [
        "## Task 2: Basic RAG Chain\n",
        "\n",
        "Now we'll set up our basic RAG chain, first up we need a model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUWXhsNVYLTA"
      },
      "source": [
        "### OpenAI Model\n",
        "\n",
        "\n",
        "We'll use OpenAI's `gpt-3.5-turbo` model to ensure we can use a stronger model for decent evaluation later!\n",
        "\n",
        "Notice that we can tag our resources - this will help us be able to keep track of which resources were used where later on!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CSgK6jgw_tI3"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "base_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", tags=[\"base_llm\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiagvgVDYTPn"
      },
      "source": [
        "#### Asyncio Bug Handling\n",
        "\n",
        "This is necessary for Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ntIqnv4cA5gR"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDO0XJqbYabb"
      },
      "source": [
        "### SiteMap Loader\n",
        "\n",
        "We'll use a SiteMapLoader to scrape the LangChain blogs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAS3QBQSARiw",
        "outputId": "c10b7cc4-e5c7-4dd8-c689-59d5c356c4d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching pages: 100%|##########| 221/221 [00:14<00:00, 14.74it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import SitemapLoader\n",
        "\n",
        "documents = SitemapLoader(web_path=\"https://blog.langchain.dev/sitemap-posts.xml\").load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_s_x87H0BYmn",
        "outputId": "2c74e338-8ba6-432f-c2da-641d5d55336e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://blog.langchain.dev/what-is-an-agent/'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0].metadata[\"source\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "221\n",
            "page_content='\\n\\n\\nWhat is an agent?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Posts\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is an agent?\\nIntroducing a new series of musings on AI agents.\\n\\nHarrison Chase\\n4 min read\\nJun 28, 2024\\n\\n\\n\\n\\n\\nâ€œWhat is an agent?â€I get asked this question almost daily. At LangChain, we build tools to help developers build LLM applications, especially those that act as a reasoning engines and interact with external sources of data and computation. This includes systems that are commonly referred to as â€œagentsâ€.Everyone seems to have a slightly different definition of what an agent is. My definition is perhaps more technical than most: ðŸ’¡An agent is a system that uses an LLM to decide the control flow of an application.Even here, Iâ€™ll admit that my definition is not perfect. People often think of agents as advanced, autonomous, and human-like â€”\\xa0but what about a simple system where an LLM routes between two different paths? This fits my technical definition, but not the common perception of what an agent should be capable of. Itâ€™s hard to define exactly what an agent is!Thatâ€™s why I really liked Andrew Ngâ€™s tweet last week. In it he suggests that â€œrather than arguing over which work to include or exclude as being a true agent, we can acknowledge that there are different degrees to which systems can be agentic.â€ Just like autonomous vehicles, for example, have levels of autonomy, we can also view agent capabilities as a spectrum. I really agree with this viewpoint and I think Andrew expressed it nicely. In the future, when I get asked about what an agent is, I will instead turn the conversation to discuss what it means to be â€œagenticâ€.What does it mean to be agentic?I gave a TED talk last year about LLM systems and used the slide below to talk about the different levels of autonomy present in LLM applications.A system is more â€œagenticâ€ the more an LLM decides how the system can behave.Using an LLM to route inputs into a particular downstream workflow has some small amount of â€œagenticâ€ behavior. This would fall into the Router category in the above diagram.If you do use multiple LLMs to do multiple routing steps? This would be somewhere between Router and State Machine.If one of those steps is then determining whether to continue or finish - effectively allowing the system to run in a loop until finished? That would fall into State Machine.If the system is building tools, remembering those, and then taking those in future steps? That is similar to what the Voyager paper implemented, and is incredibly agentic, falling into the higher Autonomous Agent category.These definitions of â€œagenticâ€ are still pretty technical. I prefer the more technical definition of â€œagenticâ€ because I think itâ€™s useful when designing and describing LLM systems.Why is â€œagenticâ€ a helpful concept?As with all concepts, itâ€™s worth asking why we even need the concept of â€œagenticâ€. What does it help with?Having an idea of how agentic your system can guide your decision-making during the development process - including building it, running it, interacting with it, evaluating it, and even monitoring it.The more agentic your system is, the more an orchestration framework will help. If you are designing a complex agentic system, having a framework with the right abstractions for thinking about these concepts can enable faster development. This framework should have first-class support for branching logic and cycles.The more agentic your system is, the harder it is to run. It will be more and more complex, having some tasks that will take a long time to complete. This means you will want to run jobs as background runs. This also means you want durable execution to handle any errors that occur halfway through.The more agentic your system is, the more you will want to interact with it while itâ€™s running. Youâ€™ll want the ability to observe what is going on inside, since the exact steps taken may not be known ahead of time. Youâ€™ll want the ability to modify the state or instructions of the agent at a particular point in time, to nudge it back on track if itâ€™s deviating from the intended path.The more agentic your system is, the more you will want an evaluation framework built for these types of applications. Youâ€™ll want to run evals multiple times, since there is compounding amount of randomness. Youâ€™ll want the ability to test not only the final output but also the intermediate steps to test how efficient the agent is behaving.The more agentic your system is, the more you will want a new type of monitoring framework. Youâ€™ll want the ability to drill down into all the steps an agent takes. Youâ€™ll also want the ability to query for runs based on steps an agent takes.Understanding and leveraging the spectrum of agentic capabilities in your system can improve the efficiency and robustness of your development process.Agentic is newOne thing that I often think about is what is actually new in all this craze. Do we need new tooling and new infrastructure for the LLM applications people are building? Or will generic tools and infrastructure from pre-LLM days suffice?To me, the more agentic your application is, the more critical it is to have new tooling and infrastructure. Thatâ€™s exactly what motivated us to build LangGraph, the agent orchestrator to help with building, running, and interacting with agents, and LangSmith, the testing and observability platform for LLM apps. As we move further on the agentic spectrum, the entire ecosystem of supportive tooling needs to be reimagined.\\n\\n\\nTags\\nHarrison ChaseIn the Loop\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            Â© LangChain Blog 2024\\n        \\n\\n\\n\\n\\n\\n\\n' metadata={'source': 'https://blog.langchain.dev/what-is-an-agent/', 'loc': 'https://blog.langchain.dev/what-is-an-agent/', 'lastmod': '2024-06-29T02:34:09.866Z'}\n"
          ]
        }
      ],
      "source": [
        "print(len(documents))\n",
        "print(documents[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F79PdFcaYfBL"
      },
      "source": [
        "### RecursiveCharacterTextSplitter\n",
        "\n",
        "We're going to use a relatively naive text splitting strategy today!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NmCdYTTTA4du"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "split_documents = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 20\n",
        ").split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLA5-LNBBVM-",
        "outputId": "f5a05c71-c382-42f4-e6a5-58f3b61d66f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1548"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUsEc07iYnwj"
      },
      "source": [
        "### Embeddings\n",
        "\n",
        "We'll be leveraging OpenAI's [text-embedding-3-small](https://openai.com/index/new-embedding-models-and-api-updates/) today!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QVhMN0aaBrsM"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "base_embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLoO_2MaY0TS"
      },
      "source": [
        "### Qdrant VectorStore Retriever\n",
        "\n",
        "Now we can use a Qdrant VectorStore to embed and store our documents and then convert it to a retriever so it can be used in our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nBTK9kSFBWM1"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import Qdrant\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    split_documents,\n",
        "    base_embeddings_model,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"langchainblogs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZpwDxlniCJRu"
      },
      "outputs": [],
      "source": [
        "base_retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2GPhHPAY5yG"
      },
      "source": [
        "### Prompt Template\n",
        "\n",
        "All we have left is a prompt template, which we'll create here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YAU74penCNmR"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "base_rag_prompt_template = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "base_rag_prompt = ChatPromptTemplate.from_template(base_rag_prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmT5VyLmZAAK"
      },
      "source": [
        "### LCEL Chain\n",
        "\n",
        "Now that we have:\n",
        "\n",
        "- Embeddings Model\n",
        "- Generation Model\n",
        "- Retriever\n",
        "- Prompt\n",
        "\n",
        "We're ready to build our LCEL chain!\n",
        "\n",
        "Keep in mind that we're returning our source documents with our queries - while this isn't necessary, it's a great thing to get into the habit of doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pqVAsUc_Cp-7"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "base_rag_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": base_rag_prompt | base_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fNjMoS-ZVo5"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6Dq9rCScDfBE",
        "outputId": "4bcebeb0-37ae-4fb8-9dae-ceba9cc1dc54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'A good way to evaluate agents is by using multiple prompts with their own instructions and few-shot examples. Each agent can even be powered by a separate fine-tuned LLM, allowing for individual evaluation and improvement without disrupting the larger application. Multi-agent designs divide complex problems into manageable units of work targeted by specialized agents and LLM programs.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is a good way to evaluate agents?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtSdDsXZXam"
      },
      "source": [
        "## Task 3: Setting Up LangSmith\n",
        "\n",
        "Now that we have a chain - we're ready to get started with LangSmith!\n",
        "\n",
        "We're going to go ahead and use the following `env` variables to get our Colab notebook set up to start reporting.\n",
        "\n",
        "If all you needed was simple monitoring - this is all you would need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iqPdBXSBD4a-"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LangSmith - {unique_id}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms4msyKLaIr6"
      },
      "source": [
        "### LangSmith API\n",
        "\n",
        "In order to use LangSmith - you will need a beta key, you can join the queue through the `Beta Sign Up` button on LangSmith's homepage!\n",
        "\n",
        "Join [here](https://www.langchain.com/langsmith)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVq1EYngEMhV",
        "outputId": "587380f0-7395-4608-aa63-35d117dbd162"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qy0MMBLacXv"
      },
      "source": [
        "Let's test our our first generation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3eoqBtBQERXP",
        "outputId": "727abc25-3510-49b7-9671-98406e672294"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'LangSmith is a unified platform for debugging, testing, evaluating, and monitoring LLM applications. It allows users to automate feedback loops, improve iteration speed, collaborate effectively, and manage their applications more efficiently.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is LangSmith?\"}, {\"tags\" : [\"Demo Run\"]})['response']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZxABFzPr2ny"
      },
      "source": [
        "## Task 4: Examining the Trace in LangSmith!\n",
        "\n",
        "Head on over to your LangSmith web UI to check out how the trace looks in LangSmith!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMzWpDK369i2"
      },
      "source": [
        "![image](https://raw.githubusercontent.com/KPGAData/AIE3/main/Week%205/LangSmith-Screenshot-1-Jul-2024.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLxh0-thanXt"
      },
      "source": [
        "## Task 5: Create Testing Dataset\n",
        "\n",
        "Now we can create a dataset using some user defined questions, and providing the retrieved context as a \"ground truth\" context.\n",
        "\n",
        "> NOTE: There are many different ways you can approach this specific task - generating ground truth answers with AI, using human experts to generate golden datasets, and more!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCKCAhASkXu0"
      },
      "source": [
        "### Synthetic Data Generation (SDG)\n",
        "\n",
        "In order to full test our RAG chain, and the various modifications we'll be using in the following notebook, we'll need to create a small synthetic dataset that is relevant to our task!\n",
        "\n",
        "Let's start by generating a series of questions - which begins with a simple model definition!\n",
        "\n",
        "> NOTE: We're going to be using a purposefully simplified datagen pipeline as an example today - but you could leverage the RAGAS SDG pipeline just as easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-dgAkRzDlEgH"
      },
      "outputs": [],
      "source": [
        "question_model = ChatOpenAI(model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv-HC4C8lWIS"
      },
      "source": [
        "Next up, we'll create some novel chunks from our source data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2Sz7rw0-lhf8"
      },
      "outputs": [],
      "source": [
        "sdg_documents = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 20\n",
        ").split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pep-eqUbllrv"
      },
      "source": [
        "Now, let's ask some questions that could be answered from the provided chunks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "e75gwn4Tlt3w"
      },
      "outputs": [],
      "source": [
        "question_prompt_template = \"\"\"\\\n",
        "You are a University Professor creating questions for an exam. You must create a question for a given piece of context.\n",
        "\n",
        "The question must be answerable only using the provided context.\n",
        "\n",
        "Avoid creating questions that are ambiguous or vague. They should be specifically related to the context.\n",
        "\n",
        "Your output must only be the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "question_prompt = ChatPromptTemplate.from_template(question_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Pcb_R-G_odCl"
      },
      "outputs": [],
      "source": [
        "question_chain = question_prompt | question_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5YbGi7umtOB"
      },
      "source": [
        "Now we can loop through a subset of our context chunks and create question/context pairs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot0HzAGBms90",
        "outputId": "0417f0df-ea17-4da0-dd3b-a6336b154c4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "question_context_pairs = []\n",
        "\n",
        "for idx in tqdm(range(0, len(sdg_documents), 40)):\n",
        "  question = question_chain.invoke({\"context\" : sdg_documents[idx].page_content})\n",
        "  question_context_pairs.append({\"question\" : question.content, \"context\" : sdg_documents[idx].page_content, \"idx\" : idx})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzhg3AJlmfW-",
        "outputId": "9c32bad2-f2db-471d-e526-ad430e427292"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': \"According to the context provided, what is being introduced in Harrison Chase's post dated Jun 28, 2024?\",\n",
              " 'context': 'What is an agent?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Posts\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is an agent?\\nIntroducing a new series of musings on AI agents.\\n\\nHarrison Chase\\n4 min read\\nJun 28, 2024',\n",
              " 'idx': 0}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_context_pairs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34zGDKtD9I0A"
      },
      "source": [
        "We'll repeat this process for answers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Ws624hGapJ92"
      },
      "outputs": [],
      "source": [
        "answer_prompt_template = \"\"\"\\\n",
        "You are a University Professor creating an exam. You must create a answer for a given piece of context and question.\n",
        "\n",
        "The answer must only rely on the provided context.\n",
        "\n",
        "Your output must only be the answer.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "answer_prompt = ChatPromptTemplate.from_template(answer_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "AEnETuSEqf2R"
      },
      "outputs": [],
      "source": [
        "answer_chain = answer_prompt | question_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89faPBcPqnfT",
        "outputId": "8e9bd52a-b400-44da-d228-34d9234c324e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:25<00:00,  1.12s/it]\n"
          ]
        }
      ],
      "source": [
        "for question_context_pair in tqdm(question_context_pairs):\n",
        "  question_context_pair[\"answer\"] = answer_chain.invoke({\"question\" : question_context_pair[\"question\"], \"context\" : question_context_pair[\"context\"]}).content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usB9PkJWq_0D",
        "outputId": "4cc1d45b-c389-4d18-a7cf-37e8efbdcf00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': \"According to the context provided, what is being introduced in Harrison Chase's post dated Jun 28, 2024?\",\n",
              " 'context': 'What is an agent?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Posts\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is an agent?\\nIntroducing a new series of musings on AI agents.\\n\\nHarrison Chase\\n4 min read\\nJun 28, 2024',\n",
              " 'idx': 0,\n",
              " 'answer': 'An agent in the context of AI is a system that can perceive its environment and take actions to achieve specific goals.'}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_context_pairs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyBtPs7p9Mbz"
      },
      "source": [
        "Now we can set up our LangSmith client - and we'll add the above created dataset to our LangSmith instance!\n",
        "\n",
        "> NOTE: Read more about this process [here](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-from-list-of-values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23\n"
          ]
        }
      ],
      "source": [
        "print(len(question_context_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "T9exE2e6F3gF"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"langsmith-demo-dataset-v1\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name, description=\"LangChain Blog Test Questions\"\n",
        ")\n",
        "\n",
        "for triplet in question_context_pairs:\n",
        "  client.create_example(\n",
        "      inputs={\"question\" : triplet[\"question\"]},\n",
        "      outputs={\"answer\" : triplet[\"answer\"]},\n",
        "      dataset_id=dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgi14vSbFIc"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Now we can run the evaluation!\n",
        "\n",
        "We'll need to start by preparing some custom data preparation functions to ensure our chain works with the expected inputs/outputs from the `evaluate` process in LangSmith.\n",
        "\n",
        "> NOTE: More reading on this available [here](https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "fbjnv3bMwQKg"
      },
      "outputs": [],
      "source": [
        "def prepare_data_ref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"reference\" : example.outputs[\"answer\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }\n",
        "\n",
        "def prepare_data_noref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuwnMdtl9nwz"
      },
      "source": [
        "We'll be using a few custom evaluators to evaluate our pipeline, as well as a few \"built in\" methods!\n",
        "\n",
        "Check out the built-ins [here](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "65d1b263094a4c6abdaf8688285c41cc",
            "087e4c759a35424385156c76e3780144",
            "18ffc7ae91d1408c8747f8d0a972f3a8",
            "8cbac09f7ba540bd8f79dde47e49fe5f",
            "b5e2be01c1834c78b7ce0cbe1f7a9d09",
            "ef2e833ac73548ee837f906d4b004a3d",
            "2c984bd9861c4ec0b27f0c3a2020dede",
            "c9927b2052be4016b0feb88ab583d32e",
            "6fbddf6663954ff48ee0cb0b4a202acb",
            "e6035d68dd9e4c6598ac5eae2d0deac1",
            "b6b1f2be05a34e118dc55869149b60ac"
          ]
        },
        "id": "CENtd4K_IQa3",
        "outputId": "528f629a-abf9-4f9b-9af4-3756aa40cc50"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Base RAG Evaluation-52b431b5' at:\n",
            "https://smith.langchain.com/o/60e92852-4998-5aa1-a58e-28b406abd32e/datasets/397eee5c-0e78-4629-a451-bd43dd42c8ba/compare?selectedSessions=27e5ef22-a9e0-4acc-a8a3-8d994ec6f156\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]Error running evaluator <DynamicRunEvaluator evaluate> on run 5cf540c9-b545-487a-bd97-a75339e0eac2: ValueError(\"Invalid output: The assistant's response cannot be evaluated for accuracy as the ground truth or reference answer is not provided. The assistant provides a specific date for the publication of the blog post, but without the correct information to compare it to, an accurate evaluation cannot be made. Therefore, no rating can be given.. Output must contain a double bracketed string                 with the verdict between 1 and 10.\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 128, in _call\n",
            "    return self.create_outputs(response)[0]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 285, in create_outputs\n",
            "    self.output_key: self.output_parser.parse_result(generation),\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/output_parsers/base.py\", line 221, in parse_result\n",
            "    return self.parse(result[0].text)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 134, in parse\n",
            "    raise ValueError(\n",
            "ValueError: Invalid output: The assistant's response cannot be evaluated for accuracy as the ground truth or reference answer is not provided. The assistant provides a specific date for the publication of the blog post, but without the correct information to compare it to, an accurate evaluation cannot be made. Therefore, no rating can be given.. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 56589c9c-17d7-49d3-98b2-92bdf1ba1a2f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9919, Requested 242. Please try again in 966ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9919, Requested 242. Please try again in 966ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d819a28c-e45e-46a5-a90a-2f119677c013: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9890, Requested 268. Please try again in 948ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9890, Requested 268. Please try again in 948ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6d6e117d-a7a9-4230-9ef9-9f428bf8b5df: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9857, Requested 310. Please try again in 1.002s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9857, Requested 310. Please try again in 1.002s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5cf540c9-b545-487a-bd97-a75339e0eac2: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9762, Requested 251. Please try again in 78ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9762, Requested 251. Please try again in 78ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 19d361a4-0bf0-4474-8277-e511df568d6d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9981, Requested 260. Please try again in 1.446s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9981, Requested 260. Please try again in 1.446s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 56589c9c-17d7-49d3-98b2-92bdf1ba1a2f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9958, Requested 283. Please try again in 1.446s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9958, Requested 283. Please try again in 1.446s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "1it [00:23, 23.68s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run cfd62e71-3ed4-4c20-8685-8a6e2ac19b37: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9844, Requested 298. Please try again in 852ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9844, Requested 298. Please try again in 852ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6bff2d8c-daf7-4243-ba68-be5b62b6b194: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9817, Requested 327. Please try again in 864ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9817, Requested 327. Please try again in 864ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6d6e117d-a7a9-4230-9ef9-9f428bf8b5df: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9796, Requested 346. Please try again in 852ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9796, Requested 346. Please try again in 852ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "2it [00:25, 10.83s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run b0536202-35bd-4f1d-a92e-929959c95616: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9947, Requested 403. Please try again in 2.1s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9947, Requested 403. Please try again in 2.1s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5cf540c9-b545-487a-bd97-a75339e0eac2: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9934, Requested 257. Please try again in 1.146s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9934, Requested 257. Please try again in 1.146s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "3it [00:25,  6.07s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 2324a228-4b3c-42da-a71d-d97953047b6c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9907, Requested 284. Please try again in 1.146s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9907, Requested 284. Please try again in 1.146s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 19d361a4-0bf0-4474-8277-e511df568d6d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9836, Requested 379. Please try again in 1.29s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9836, Requested 379. Please try again in 1.29s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "4it [00:27,  4.46s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 38313ce9-ddde-4ec0-a513-26bef16c22d9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9812, Requested 406. Please try again in 1.308s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9812, Requested 406. Please try again in 1.308s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 475611b0-625d-43d1-a5f6-42322ce608ca: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9966, Requested 358. Please try again in 1.944s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9966, Requested 358. Please try again in 1.944s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cfd62e71-3ed4-4c20-8685-8a6e2ac19b37: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9961, Requested 359. Please try again in 1.92s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9961, Requested 359. Please try again in 1.92s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "5it [00:29,  3.35s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 6bff2d8c-daf7-4243-ba68-be5b62b6b194: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9939, Requested 382. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9939, Requested 382. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 97ab8965-94b8-443a-9a1f-f9566d680c4c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9940, Requested 364. Please try again in 1.824s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9940, Requested 364. Please try again in 1.824s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "7it [00:29,  1.70s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run b0536202-35bd-4f1d-a92e-929959c95616: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9824, Requested 497. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9824, Requested 497. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "8it [00:30,  1.39s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 38313ce9-ddde-4ec0-a513-26bef16c22d9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9946, Requested 328. Please try again in 1.644s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9946, Requested 328. Please try again in 1.644s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bf82ef32-ebd2-4d8f-966c-14899c94812e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9871, Requested 364. Please try again in 1.41s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9871, Requested 364. Please try again in 1.41s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 475611b0-625d-43d1-a5f6-42322ce608ca: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9973, Requested 266. Please try again in 1.434s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9973, Requested 266. Please try again in 1.434s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 77bdf0be-2b28-415d-b44f-e2593925d08f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9904, Requested 333. Please try again in 1.422s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9904, Requested 333. Please try again in 1.422s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b01897b8-afd5-4e5f-9acd-97ba774565a7: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9912, Requested 327. Please try again in 1.434s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9912, Requested 327. Please try again in 1.434s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b166eb29-a95e-45bb-889f-1c4c608cbc81: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9877, Requested 360. Please try again in 1.422s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9877, Requested 360. Please try again in 1.422s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 831d4a4d-7d0d-4828-a8a5-0ee8f5a64eed: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9801, Requested 428. Please try again in 1.374s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9801, Requested 428. Please try again in 1.374s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bf82ef32-ebd2-4d8f-966c-14899c94812e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9956, Requested 270. Please try again in 1.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9956, Requested 270. Please try again in 1.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 38313ce9-ddde-4ec0-a513-26bef16c22d9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9880, Requested 345. Please try again in 1.35s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9880, Requested 345. Please try again in 1.35s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 475611b0-625d-43d1-a5f6-42322ce608ca: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9934, Requested 297. Please try again in 1.386s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9934, Requested 297. Please try again in 1.386s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b166eb29-a95e-45bb-889f-1c4c608cbc81: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9930, Requested 301. Please try again in 1.386s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9930, Requested 301. Please try again in 1.386s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 77bdf0be-2b28-415d-b44f-e2593925d08f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9969, Requested 265. Please try again in 1.404s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9969, Requested 265. Please try again in 1.404s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d819a28c-e45e-46a5-a90a-2f119677c013: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9961, Requested 283. Please try again in 1.464s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9961, Requested 283. Please try again in 1.464s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "9it [00:37,  2.94s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run bf82ef32-ebd2-4d8f-966c-14899c94812e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9995, Requested 302. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9995, Requested 302. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 38313ce9-ddde-4ec0-a513-26bef16c22d9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9971, Requested 325. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9971, Requested 325. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 831d4a4d-7d0d-4828-a8a5-0ee8f5a64eed: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9948, Requested 347. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9948, Requested 347. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "11it [00:38,  1.76s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 77bdf0be-2b28-415d-b44f-e2593925d08f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9862, Requested 271. Please try again in 798ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9862, Requested 271. Please try again in 798ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b166eb29-a95e-45bb-889f-1c4c608cbc81: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9784, Requested 298. Please try again in 492ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9784, Requested 298. Please try again in 492ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bf82ef32-ebd2-4d8f-966c-14899c94812e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9783, Requested 267. Please try again in 300ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9783, Requested 267. Please try again in 300ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "12it [00:39,  1.64s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 97ab8965-94b8-443a-9a1f-f9566d680c4c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9945, Requested 303. Please try again in 1.488s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9945, Requested 303. Please try again in 1.488s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c10f907b-0186-4064-9d45-23368aeffd58: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9800, Requested 441. Please try again in 1.446s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9800, Requested 441. Please try again in 1.446s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b166eb29-a95e-45bb-889f-1c4c608cbc81: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9932, Requested 298. Please try again in 1.38s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9932, Requested 298. Please try again in 1.38s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 831d4a4d-7d0d-4828-a8a5-0ee8f5a64eed: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9865, Requested 367. Please try again in 1.392s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9865, Requested 367. Please try again in 1.392s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 38313ce9-ddde-4ec0-a513-26bef16c22d9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9864, Requested 360. Please try again in 1.344s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9864, Requested 360. Please try again in 1.344s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "13it [00:42,  1.94s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run f0cbbf16-9c47-46a1-a53d-4103d49fd3e3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9827, Requested 400. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9827, Requested 400. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f84d6b1e-8b2d-4aae-8ba7-797bb1aacbbf: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9779, Requested 444. Please try again in 1.338s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9779, Requested 444. Please try again in 1.338s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bf82ef32-ebd2-4d8f-966c-14899c94812e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9898, Requested 318. Please try again in 1.296s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9898, Requested 318. Please try again in 1.296s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "14it [00:43,  1.75s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run c10f907b-0186-4064-9d45-23368aeffd58: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9968, Requested 312. Please try again in 1.68s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9968, Requested 312. Please try again in 1.68s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b166eb29-a95e-45bb-889f-1c4c608cbc81: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9968, Requested 313. Please try again in 1.686s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9968, Requested 313. Please try again in 1.686s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "15it [00:44,  1.68s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 831d4a4d-7d0d-4828-a8a5-0ee8f5a64eed: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9947, Requested 345. Please try again in 1.752s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9947, Requested 345. Please try again in 1.752s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b01897b8-afd5-4e5f-9acd-97ba774565a7: ValueError(\"Invalid output: The assistant's response cannot be evaluated for accuracy as the ground truth or reference answer is not provided. The assistant's response seems to be relevant to the question, but without the reference answer, it's impossible to determine its accuracy. Therefore, I cannot provide a rating.. Output must contain a double bracketed string                 with the verdict between 1 and 10.\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 128, in _call\n",
            "    return self.create_outputs(response)[0]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 285, in create_outputs\n",
            "    self.output_key: self.output_parser.parse_result(generation),\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/output_parsers/base.py\", line 221, in parse_result\n",
            "    return self.parse(result[0].text)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 134, in parse\n",
            "    raise ValueError(\n",
            "ValueError: Invalid output: The assistant's response cannot be evaluated for accuracy as the ground truth or reference answer is not provided. The assistant's response seems to be relevant to the question, but without the reference answer, it's impossible to determine its accuracy. Therefore, I cannot provide a rating.. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 475611b0-625d-43d1-a5f6-42322ce608ca: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9922, Requested 312. Please try again in 1.404s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9922, Requested 312. Please try again in 1.404s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "16it [00:46,  1.72s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run f0cbbf16-9c47-46a1-a53d-4103d49fd3e3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9912, Requested 315. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9912, Requested 315. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c10f907b-0186-4064-9d45-23368aeffd58: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9699, Requested 380. Please try again in 474ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9699, Requested 380. Please try again in 474ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 831d4a4d-7d0d-4828-a8a5-0ee8f5a64eed: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9983, Requested 382. Please try again in 2.19s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9983, Requested 382. Please try again in 2.19s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "17it [00:48,  1.80s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run f0cbbf16-9c47-46a1-a53d-4103d49fd3e3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9955, Requested 338. Please try again in 1.758s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9955, Requested 338. Please try again in 1.758s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 97ab8965-94b8-443a-9a1f-f9566d680c4c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9726, Requested 318. Please try again in 264ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9726, Requested 318. Please try again in 264ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "19it [00:54,  2.33s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run f0cbbf16-9c47-46a1-a53d-4103d49fd3e3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9787, Requested 313. Please try again in 600ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9787, Requested 313. Please try again in 600ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f84d6b1e-8b2d-4aae-8ba7-797bb1aacbbf: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9682, Requested 383. Please try again in 390ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9682, Requested 383. Please try again in 390ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "23it [01:21,  3.54s/it]\n"
          ]
        }
      ],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", prepare_data=prepare_data_ref)\n",
        "\n",
        "unlabeled_dopeness_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"criteria\" : {\n",
        "            \"dopeness\" : \"Is the answer to the question dope, meaning cool - awesome - and legit?\"\n",
        "        }\n",
        "    },\n",
        "    prepare_data=prepare_data_noref\n",
        ")\n",
        "\n",
        "labeled_score_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_score_string\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"accuracy\": \"Is the generated answer the same as the reference answer?\"\n",
        "        },\n",
        "    },\n",
        "    prepare_data=prepare_data_ref\n",
        ")\n",
        "\n",
        "unlabeled_coherence_evaluator = LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"coherence\"}, prepare_data=prepare_data_noref)\n",
        "labeled_relevance_evaluator = LangChainStringEvaluator(\"labeled_criteria\", config={ \"criteria\": \"relevance\"}, prepare_data=prepare_data_ref)\n",
        "\n",
        "base_rag_results = evaluate(\n",
        "    base_rag_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        unlabeled_coherence_evaluator,\n",
        "        labeled_relevance_evaluator\n",
        "        ],\n",
        "    experiment_prefix=\"Base RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwhBxlxYAdno"
      },
      "source": [
        "## Testing Other Retrievers\n",
        "\n",
        "Now we can test our how changing our Retriever impacts our LangSmith evaluation!\n",
        "\n",
        "We'll build this simple qa_chain factory to create standardized qa_chains where the only different component will be the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qnfy4VNkzZi2"
      },
      "outputs": [],
      "source": [
        "def create_qa_chain(retriever):\n",
        "  primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "  created_qa_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever,\n",
        "     \"question\": itemgetter(\"question\")\n",
        "    }\n",
        "    | RunnablePassthrough.assign(\n",
        "        context=itemgetter(\"context\")\n",
        "      )\n",
        "    | {\n",
        "         \"response\": base_rag_prompt | primary_qa_llm,\n",
        "         \"context\": itemgetter(\"context\"),\n",
        "      }\n",
        "  )\n",
        "  return created_qa_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOPp4Xq7AvEx"
      },
      "source": [
        "### Task 1: Parent Document Retriever\n",
        "\n",
        "One of the easier ways we can imagine improving a retriever is to embed our documents into small chunks, and then retrieve a significant amount of additional context that \"surrounds\" the found context.\n",
        "\n",
        "You can read more about this method [here](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever)!\n",
        "\n",
        "The basic outline of this retrieval method is as follows:\n",
        "\n",
        "1. Obtain User Question\n",
        "2. Retrieve child documents using Dense Vector Retrieval\n",
        "3. Merge the child documents based on their parents. If they have the same parents - they become merged.\n",
        "4. Replace the child documents with their respective parent documents from an in-memory-store.\n",
        "5. Use the parent documents to augment generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "67I6QJAJ0Un7"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams\n",
        "\n",
        "client = QdrantClient(\":memory:\")\n",
        "client.create_collection(\n",
        "    collection_name=\"split_parents\",\n",
        "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "vectorstore = Qdrant(client, collection_name=\"split_parents\", embeddings=base_embeddings_model)\n",
        "\n",
        "store = InMemoryStore()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "zfk5RYUt00Pw"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=RecursiveCharacterTextSplitter(chunk_size=400),\n",
        "    parent_splitter=RecursiveCharacterTextSplitter(chunk_size=2000),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "68c1t4o104AK"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTH0MDolBndm"
      },
      "source": [
        "Let's create, test, and then evaluate our new chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "KMjLfqOC09Iw"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever_qa_chain = create_qa_chain(parent_document_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Rv8bAHPN1H4P",
        "outputId": "46487575-438d-4c12-d306-7a4341d7ed83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval Augmented Generation, which is a central paradigm in Large Language Model (LLM) application development. It involves connecting LLMs to external data sources to provide recent and private information for training. The basic RAG pipeline includes embedding a user query, retrieving relevant documents, and passing them to an LLM for generating an answer based on the retrieved context.'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retriever_qa_chain.invoke({\"question\" : \"What is RAG?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6H7vHCt2HJi"
      },
      "source": [
        "#### Evaluating the Parent Document Retrieval Pipeline\n",
        "\n",
        "Now that we've created a new retriever - let's try evaluating it on the same dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "142e5756a6c840aeb25b733ece015ed2",
            "4777a17d7d9c444a8d44e070faef0ad0",
            "dad645f3259c41b5a50e0e94906ca3e3",
            "c67428191d074e4d80cb0f28fb65cd41",
            "8db8082c5ab741b5b4fd78027e65135d",
            "2ab8452345f24b46b172515ddc77fdb0",
            "d5ad942ac5ed41c6a581b4022f177114",
            "27e2bff47a084ed6bc7d96ac09af9ebc",
            "d097ee02256d40d5b395d37cd0face05",
            "13c75ca5ee9d42eb8476a307f30b7528",
            "e8143f8ceee6468c98433734ff59b835"
          ]
        },
        "id": "Z-0WFCtx2N4n",
        "outputId": "341b1af3-cd74-46a9-d9d1-5ab2190ec96c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Parent Document Retrieval RAG Evaluation-3cfb5964' at:\n",
            "https://smith.langchain.com/o/60e92852-4998-5aa1-a58e-28b406abd32e/datasets/397eee5c-0e78-4629-a451-bd43dd42c8ba/compare?selectedSessions=7a9dfd4c-f96d-41f7-b51e-2731f65f3831\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]Error running evaluator <DynamicRunEvaluator evaluate> on run c121cadd-782a-456c-8b62-ec394d6fad64: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9716, Requested 327. Please try again in 258ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9716, Requested 327. Please try again in 258ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f5b8415c-e27c-40f9-b6fd-c63aa27b7cbf: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9898, Requested 429. Please try again in 1.962s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9898, Requested 429. Please try again in 1.962s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d0451492-24f8-4124-a259-e28e40c84314: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9911, Requested 315. Please try again in 1.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9911, Requested 315. Please try again in 1.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1b21e0bc-779c-458d-a72a-6057f3702b81: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9807, Requested 395. Please try again in 1.212s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9807, Requested 395. Please try again in 1.212s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 27ccfa43-d357-44c3-9efd-29cac4cf3f07: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9824, Requested 369. Please try again in 1.158s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9824, Requested 369. Please try again in 1.158s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 93bc1460-eea2-4fc4-8c17-8b7010d4db09: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9816, Requested 418. Please try again in 1.404s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9816, Requested 418. Please try again in 1.404s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8331d9e3-16c3-459a-9404-34d672bbe726: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9681, Requested 365. Please try again in 276ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9681, Requested 365. Please try again in 276ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cc6db052-f5aa-44cc-8c5c-657348c22ab7: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9930, Requested 339. Please try again in 1.614s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9930, Requested 339. Please try again in 1.614s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c121cadd-782a-456c-8b62-ec394d6fad64: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9927, Requested 334. Please try again in 1.566s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9927, Requested 334. Please try again in 1.566s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f5b8415c-e27c-40f9-b6fd-c63aa27b7cbf: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9856, Requested 383. Please try again in 1.434s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9856, Requested 383. Please try again in 1.434s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e5965f91-04d7-4c99-9600-b7660d14dca9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9922, Requested 349. Please try again in 1.626s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9922, Requested 349. Please try again in 1.626s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 27ccfa43-d357-44c3-9efd-29cac4cf3f07: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9920, Requested 372. Please try again in 1.752s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9920, Requested 372. Please try again in 1.752s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 93bc1460-eea2-4fc4-8c17-8b7010d4db09: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9854, Requested 397. Please try again in 1.506s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9854, Requested 397. Please try again in 1.506s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1b21e0bc-779c-458d-a72a-6057f3702b81: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9852, Requested 376. Please try again in 1.368s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9852, Requested 376. Please try again in 1.368s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8331d9e3-16c3-459a-9404-34d672bbe726: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9959, Requested 355. Please try again in 1.883s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9959, Requested 355. Please try again in 1.883s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 991ede71-3a98-4d9c-93ad-c3cf335c619a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9911, Requested 428. Please try again in 2.034s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9911, Requested 428. Please try again in 2.034s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 31666ebe-7e5b-4869-a82b-c6dea1c49d96: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9924, Requested 371. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9924, Requested 371. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "2it [00:52, 21.63s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run e5965f91-04d7-4c99-9600-b7660d14dca9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9920, Requested 468. Please try again in 2.328s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9920, Requested 468. Please try again in 2.328s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "4it [00:52,  7.38s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run f5b8415c-e27c-40f9-b6fd-c63aa27b7cbf: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9738, Requested 444. Please try again in 1.092s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9738, Requested 444. Please try again in 1.092s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 93bc1460-eea2-4fc4-8c17-8b7010d4db09: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9790, Requested 434. Please try again in 1.344s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9790, Requested 434. Please try again in 1.344s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "6it [00:53,  3.65s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 1b21e0bc-779c-458d-a72a-6057f3702b81: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9768, Requested 410. Please try again in 1.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9768, Requested 410. Please try again in 1.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Failed to batch ingest runs: LangSmithConnectionError(\"Connection error caused failure to POST https://api.smith.langchain.com/runs/batch  in LangSmith API. Please confirm your internet connection.. ConnectionError(ProtocolError('Connection aborted.', TimeoutError('The write operation timed out')))\")\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8331d9e3-16c3-459a-9404-34d672bbe726: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9932, Requested 380. Please try again in 1.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9932, Requested 380. Please try again in 1.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "9it [00:55,  2.06s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 991ede71-3a98-4d9c-93ad-c3cf335c619a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9741, Requested 522. Please try again in 1.578s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9741, Requested 522. Please try again in 1.578s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "12it [01:02,  2.19s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 605aea4c-a650-4a80-9006-7cfe332a7117: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9897, Requested 452. Please try again in 2.094s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9897, Requested 452. Please try again in 2.094s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dc46aaec-bd4c-4862-950d-6243f77b114e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9802, Requested 607. Please try again in 2.454s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9802, Requested 607. Please try again in 2.454s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b6049d6b-7aa4-4724-8040-e1b5bd79cfec: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9714, Requested 514. Please try again in 1.368s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9714, Requested 514. Please try again in 1.368s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f89ad182-87ba-4f8d-b231-0334843d4221: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9630, Requested 460. Please try again in 540ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9630, Requested 460. Please try again in 540ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 676e57f7-320f-41c1-82c9-a76eb0cc4767: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9634, Requested 477. Please try again in 666ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9634, Requested 477. Please try again in 666ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 838ab451-288c-46a5-b553-69f1ed3f88e6: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9633, Requested 452. Please try again in 510ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9633, Requested 452. Please try again in 510ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 09641533-a13e-4db0-8969-8cd4cfd23a99: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9756, Requested 522. Please try again in 1.668s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9756, Requested 522. Please try again in 1.668s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7a1b887d-2f27-453b-a8a6-99c4ec5e9cbf: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9768, Requested 420. Please try again in 1.128s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9768, Requested 420. Please try again in 1.128s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b6049d6b-7aa4-4724-8040-e1b5bd79cfec: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9751, Requested 385. Please try again in 816ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9751, Requested 385. Please try again in 816ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dc46aaec-bd4c-4862-950d-6243f77b114e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9636, Requested 463. Please try again in 594ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9636, Requested 463. Please try again in 594ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 09641533-a13e-4db0-8969-8cd4cfd23a99: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9699, Requested 441. Please try again in 840ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9699, Requested 441. Please try again in 840ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2d85e7e9-cbdd-43ad-adab-057b4d72256e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9775, Requested 342. Please try again in 702ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9775, Requested 342. Please try again in 702ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7a1b887d-2f27-453b-a8a6-99c4ec5e9cbf: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9891, Requested 352. Please try again in 1.458s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9891, Requested 352. Please try again in 1.458s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b6049d6b-7aa4-4724-8040-e1b5bd79cfec: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9745, Requested 453. Please try again in 1.188s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9745, Requested 453. Please try again in 1.188s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7a1b887d-2f27-453b-a8a6-99c4ec5e9cbf: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9682, Requested 358. Please try again in 240ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9682, Requested 358. Please try again in 240ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 605aea4c-a650-4a80-9006-7cfe332a7117: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9795, Requested 391. Please try again in 1.116s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9795, Requested 391. Please try again in 1.116s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dc46aaec-bd4c-4862-950d-6243f77b114e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9611, Requested 546. Please try again in 941ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9611, Requested 546. Please try again in 941ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Failed to batch ingest runs: LangSmithConnectionError(\"Connection error caused failure to POST https://api.smith.langchain.com/runs/batch  in LangSmith API. Please confirm your internet connection.. ConnectionError(ProtocolError('Connection aborted.', TimeoutError('The write operation timed out')))\")\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 88152f4b-8a2e-46c1-bb0e-5f7e010aa2b2: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9954, Requested 386. Please try again in 2.04s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9954, Requested 386. Please try again in 2.04s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 838ab451-288c-46a5-b553-69f1ed3f88e6: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9657, Requested 390. Please try again in 282ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9657, Requested 390. Please try again in 282ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 09641533-a13e-4db0-8969-8cd4cfd23a99: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9719, Requested 460. Please try again in 1.074s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9719, Requested 460. Please try again in 1.074s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b6049d6b-7aa4-4724-8040-e1b5bd79cfec: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9753, Requested 382. Please try again in 810ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9753, Requested 382. Please try again in 810ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 676e57f7-320f-41c1-82c9-a76eb0cc4767: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9802, Requested 416. Please try again in 1.308s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9802, Requested 416. Please try again in 1.308s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fcacfb9e-c56a-444a-84e9-3ea57192030e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9817, Requested 354. Please try again in 1.026s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9817, Requested 354. Please try again in 1.026s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 88152f4b-8a2e-46c1-bb0e-5f7e010aa2b2: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9619, Requested 385. Please try again in 24ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9619, Requested 385. Please try again in 24ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fcacfb9e-c56a-444a-84e9-3ea57192030e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9972, Requested 335. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9972, Requested 335. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 09641533-a13e-4db0-8969-8cd4cfd23a99: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9626, Requested 438. Please try again in 384ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9626, Requested 438. Please try again in 384ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b6049d6b-7aa4-4724-8040-e1b5bd79cfec: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9944, Requested 468. Please try again in 2.472s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9944, Requested 468. Please try again in 2.472s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "13it [01:44, 13.45s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 7a1b887d-2f27-453b-a8a6-99c4ec5e9cbf: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9647, Requested 373. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9647, Requested 373. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "14it [01:47, 10.42s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 838ab451-288c-46a5-b553-69f1ed3f88e6: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9661, Requested 355. Please try again in 96ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9661, Requested 355. Please try again in 96ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 88152f4b-8a2e-46c1-bb0e-5f7e010aa2b2: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9888, Requested 401. Please try again in 1.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9888, Requested 401. Please try again in 1.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "15it [01:48,  7.61s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run f89ad182-87ba-4f8d-b231-0334843d4221: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9878, Requested 413. Please try again in 1.746s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9878, Requested 413. Please try again in 1.746s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "16it [01:50,  6.20s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 09641533-a13e-4db0-8969-8cd4cfd23a99: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9809, Requested 476. Please try again in 1.71s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9809, Requested 476. Please try again in 1.71s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "17it [01:51,  4.49s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 2d85e7e9-cbdd-43ad-adab-057b4d72256e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9836, Requested 380. Please try again in 1.296s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9836, Requested 380. Please try again in 1.296s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "18it [01:52,  3.44s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 838ab451-288c-46a5-b553-69f1ed3f88e6: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9717, Requested 406. Please try again in 738ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9717, Requested 406. Please try again in 738ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "20it [01:53,  2.02s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run dc46aaec-bd4c-4862-950d-6243f77b114e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9715, Requested 561. Please try again in 1.656s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9715, Requested 561. Please try again in 1.656s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "23it [02:04,  5.41s/it]\n"
          ]
        }
      ],
      "source": [
        "pdr_rag_results = evaluate(\n",
        "    parent_document_retriever_qa_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        unlabeled_coherence_evaluator,\n",
        "        labeled_relevance_evaluator\n",
        "        ],\n",
        "    experiment_prefix=\"Parent Document Retrieval RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaNk6o7_BqX8"
      },
      "source": [
        "### Task 2: Ensemble Retrieval\n",
        "\n",
        "Next let's look at ensemble retrieval!\n",
        "\n",
        "You can read more about this [here](https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble)!\n",
        "\n",
        "The basic idea is as follows:\n",
        "\n",
        "1. Obtain User Question\n",
        "2. Hit the Retriever Pair\n",
        "    - Retrieve Documents with BM25 Sparse Vector Retrieval\n",
        "    - Retrieve Documents with Dense Vector Retrieval Method\n",
        "3. Collect and \"fuse\" the retrieved docs based on their weighting using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm into a single ranked list.\n",
        "4. Use those documents to augment our generation.\n",
        "\n",
        "Ensure your `weights` list - the relative weighting of each retriever - sums to 1!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "zz7dl1GD5-L-"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Vs8wxT9b5pRA"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=450, chunk_overlap=75)\n",
        "split_documents = text_splitter.split_documents(documents)\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(split_documents)\n",
        "bm25_retriever.k = 2\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectorstore = Qdrant.from_documents(split_documents, embedding, location=\":memory:\")\n",
        "qdrant_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[\n",
        "        bm25_retriever,\n",
        "        qdrant_retriever\n",
        "    ],\n",
        "    weights=[\n",
        "        0.5,\n",
        "        0.5\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "cv69YDpF6PrJ"
      },
      "outputs": [],
      "source": [
        "ensemble_retriever_qa_chain = create_qa_chain(ensemble_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6lSszzrf6UmP",
        "outputId": "ea13ffbc-df0f-4191-f873-6c2f0405d874"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval Augmented Generation, which is a central paradigm in LLM (Large Language Models) application development that connects LLMs to external data sources to address the lack of recent or private information in their training data.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retriever_qa_chain.invoke({\"question\" : \"What is RAG?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "e99b5317081e42c199a297f3a483bbf5",
            "42bfd25ca7a14d629e4070b093a197d5",
            "fa9f9fc79b344271adc94acc487aef59",
            "a603cf6ec7d544a1a951fb8defccb976",
            "7448f6f5d9dc431fba689d8821285ca3",
            "1d0589c63fd1461b93b403c594b6ccfa",
            "5e500672dafc4529806e4e5f72fe97fa",
            "ece363bdc8b54631beba2628e3175e0b",
            "46a7a454e9bb486588ac33156abddecc",
            "f79f87ac933b45fa9e2c17871d4f2e44",
            "a888bb92387549fea8843f20e3b4c50f"
          ]
        },
        "id": "GVBY5lhm4KG7",
        "outputId": "d48d2604-1ac3-4b8a-c4aa-93214b4c0e06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Hybrid Retrieval RAG Evaluation-19fea280' at:\n",
            "https://smith.langchain.com/o/60e92852-4998-5aa1-a58e-28b406abd32e/datasets/397eee5c-0e78-4629-a451-bd43dd42c8ba/compare?selectedSessions=bc131a9e-28b3-4c5d-8b55-eca282a564f4\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]Error running evaluator <DynamicRunEvaluator evaluate> on run 2ec76ee1-678c-4d11-952d-b2fa7d2f7848: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9875, Requested 328. Please try again in 1.218s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9875, Requested 328. Please try again in 1.218s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5fd7f27e-d168-4ae8-b991-3d50fd3fc5fb: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9951, Requested 442. Please try again in 2.358s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9951, Requested 442. Please try again in 2.358s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 35a8eb4a-1a77-4ae4-b3c9-32da286c808e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9967, Requested 327. Please try again in 1.764s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9967, Requested 327. Please try again in 1.764s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2ec76ee1-678c-4d11-952d-b2fa7d2f7848: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9963, Requested 337. Please try again in 1.8s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9963, Requested 337. Please try again in 1.8s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fc1a0979-1a44-40cb-abb3-f66de34ae95d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9953, Requested 383. Please try again in 2.015s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9953, Requested 383. Please try again in 2.015s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 50b7ecb3-6eed-432a-acf0-d4ad9806e3d5: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9956, Requested 345. Please try again in 1.806s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9956, Requested 345. Please try again in 1.806s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 26d6ab52-8387-45ce-b894-bd9dea933a3b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9935, Requested 364. Please try again in 1.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9935, Requested 364. Please try again in 1.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7644e447-b6cd-4b03-9704-19bb35f28496: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9928, Requested 367. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9928, Requested 367. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f08dcea2-5e7d-49ad-aa6a-fcdffe08d0df: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9903, Requested 397. Please try again in 1.8s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9903, Requested 397. Please try again in 1.8s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4ac10e26-c7a4-4408-b139-9d81e96f144b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9886, Requested 412. Please try again in 1.788s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9886, Requested 412. Please try again in 1.788s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5fd7f27e-d168-4ae8-b991-3d50fd3fc5fb: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9969, Requested 338. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9969, Requested 338. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2ec76ee1-678c-4d11-952d-b2fa7d2f7848: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9965, Requested 343. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9965, Requested 343. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "1it [00:31, 31.91s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run fc1a0979-1a44-40cb-abb3-f66de34ae95d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9965, Requested 343. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9965, Requested 343. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dad4391f-19a2-4c29-a9cd-88c184239b8d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9886, Requested 360. Please try again in 1.476s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9886, Requested 360. Please try again in 1.476s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7644e447-b6cd-4b03-9704-19bb35f28496: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9940, Requested 370. Please try again in 1.86s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9940, Requested 370. Please try again in 1.86s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4ac10e26-c7a4-4408-b139-9d81e96f144b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9946, Requested 366. Please try again in 1.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9946, Requested 366. Please try again in 1.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 50b7ecb3-6eed-432a-acf0-d4ad9806e3d5: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9941, Requested 370. Please try again in 1.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9941, Requested 370. Please try again in 1.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "2it [00:34, 14.42s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run f08dcea2-5e7d-49ad-aa6a-fcdffe08d0df: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9844, Requested 433. Please try again in 1.662s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9844, Requested 433. Please try again in 1.662s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "3it [00:34,  8.15s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run fc1a0979-1a44-40cb-abb3-f66de34ae95d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9698, Requested 398. Please try again in 576ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9698, Requested 398. Please try again in 576ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "4it [00:36,  5.56s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run dad4391f-19a2-4c29-a9cd-88c184239b8d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9701, Requested 395. Please try again in 576ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9701, Requested 395. Please try again in 576ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5fd7f27e-d168-4ae8-b991-3d50fd3fc5fb: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9924, Requested 457. Please try again in 2.286s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9924, Requested 457. Please try again in 2.286s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "6it [00:36,  2.76s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 7644e447-b6cd-4b03-9704-19bb35f28496: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9890, Requested 382. Please try again in 1.632s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9890, Requested 382. Please try again in 1.632s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "7it [00:37,  2.21s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run ce1954d1-e6ef-42a5-8a25-e1c86b818774: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9797, Requested 581. Please try again in 2.268s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9797, Requested 581. Please try again in 2.268s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4ac10e26-c7a4-4408-b139-9d81e96f144b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9647, Requested 427. Please try again in 444ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9647, Requested 427. Please try again in 444ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "8it [00:39,  2.01s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 51618049-a5eb-487f-b050-9b0c2934c972: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9939, Requested 451. Please try again in 2.34s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9939, Requested 451. Please try again in 2.34s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dd523fbc-d01a-4727-92b3-60374a78a27e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9952, Requested 453. Please try again in 2.43s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9952, Requested 453. Please try again in 2.43s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 67f26670-2afb-4c67-b117-929347ff8ee6: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9922, Requested 420. Please try again in 2.052s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9922, Requested 420. Please try again in 2.052s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "9it [00:42,  2.43s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 833ec5c6-cd64-47d2-b69c-43c114b7813e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9756, Requested 514. Please try again in 1.62s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9756, Requested 514. Please try again in 1.62s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9c28d6b4-5922-4e0a-b1af-cff98b8d67f3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9718, Requested 447. Please try again in 990ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9718, Requested 447. Please try again in 990ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ce1954d1-e6ef-42a5-8a25-e1c86b818774: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9990, Requested 444. Please try again in 2.604s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9990, Requested 444. Please try again in 2.604s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 26d6ab52-8387-45ce-b894-bd9dea933a3b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9820, Requested 379. Please try again in 1.194s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9820, Requested 379. Please try again in 1.194s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "10it [00:45,  2.48s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run f0da1801-28ce-4e8f-ba6d-88bfea86d95e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9802, Requested 415. Please try again in 1.302s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9802, Requested 415. Please try again in 1.302s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "11it [00:46,  2.23s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 36ae4cb2-6245-4efd-a1bc-8ad916ba52fc: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9638, Requested 451. Please try again in 534ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9638, Requested 451. Please try again in 534ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 51618049-a5eb-487f-b050-9b0c2934c972: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9911, Requested 339. Please try again in 1.5s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9911, Requested 339. Please try again in 1.5s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "12it [00:48,  2.08s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 833ec5c6-cd64-47d2-b69c-43c114b7813e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9841, Requested 385. Please try again in 1.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9841, Requested 385. Please try again in 1.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 94c66ac6-365d-4ada-aac3-270122668698: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9803, Requested 504. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9803, Requested 504. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e1790b35-ef47-49f3-822c-dda7f981db04: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9739, Requested 484. Please try again in 1.338s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9739, Requested 484. Please try again in 1.338s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 36ae4cb2-6245-4efd-a1bc-8ad916ba52fc: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9845, Requested 357. Please try again in 1.212s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9845, Requested 357. Please try again in 1.212s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 51618049-a5eb-487f-b050-9b0c2934c972: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9758, Requested 390. Please try again in 888ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9758, Requested 390. Please try again in 888ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9b1e9983-95db-4c02-8792-46a9117754c8: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9669, Requested 543. Please try again in 1.272s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9669, Requested 543. Please try again in 1.272s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ce1954d1-e6ef-42a5-8a25-e1c86b818774: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9730, Requested 519. Please try again in 1.494s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9730, Requested 519. Please try again in 1.494s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 833ec5c6-cd64-47d2-b69c-43c114b7813e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9740, Requested 453. Please try again in 1.158s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9740, Requested 453. Please try again in 1.158s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e1790b35-ef47-49f3-822c-dda7f981db04: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9623, Requested 400. Please try again in 138ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9623, Requested 400. Please try again in 138ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dd523fbc-d01a-4727-92b3-60374a78a27e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9898, Requested 392. Please try again in 1.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9898, Requested 392. Please try again in 1.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9b1e9983-95db-4c02-8792-46a9117754c8: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9935, Requested 399. Please try again in 2.004s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9935, Requested 399. Please try again in 2.004s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ce1954d1-e6ef-42a5-8a25-e1c86b818774: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9948, Requested 441. Please try again in 2.334s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9948, Requested 441. Please try again in 2.334s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 833ec5c6-cd64-47d2-b69c-43c114b7813e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9821, Requested 382. Please try again in 1.218s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9821, Requested 382. Please try again in 1.218s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e1790b35-ef47-49f3-822c-dda7f981db04: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9908, Requested 423. Please try again in 1.985s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9908, Requested 423. Please try again in 1.985s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9c28d6b4-5922-4e0a-b1af-cff98b8d67f3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9907, Requested 385. Please try again in 1.752s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9907, Requested 385. Please try again in 1.752s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9b1e9983-95db-4c02-8792-46a9117754c8: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9829, Requested 481. Please try again in 1.86s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9829, Requested 481. Please try again in 1.86s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dd523fbc-d01a-4727-92b3-60374a78a27e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9663, Requested 358. Please try again in 125ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9663, Requested 358. Please try again in 125ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9c28d6b4-5922-4e0a-b1af-cff98b8d67f3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9864, Requested 385. Please try again in 1.494s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9864, Requested 385. Please try again in 1.494s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 36ae4cb2-6245-4efd-a1bc-8ad916ba52fc: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9802, Requested 405. Please try again in 1.242s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9802, Requested 405. Please try again in 1.242s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "13it [01:11,  8.17s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 51618049-a5eb-487f-b050-9b0c2934c972: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9805, Requested 405. Please try again in 1.26s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9805, Requested 405. Please try again in 1.26s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "14it [01:11,  5.77s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 94c66ac6-365d-4ada-aac3-270122668698: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9792, Requested 442. Please try again in 1.404s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9792, Requested 442. Please try again in 1.404s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e1790b35-ef47-49f3-822c-dda7f981db04: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9729, Requested 397. Please try again in 756ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9729, Requested 397. Please try again in 756ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 833ec5c6-cd64-47d2-b69c-43c114b7813e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9676, Requested 468. Please try again in 864ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9676, Requested 468. Please try again in 864ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "15it [01:11,  4.15s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run ce1954d1-e6ef-42a5-8a25-e1c86b818774: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9673, Requested 535. Please try again in 1.248s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9673, Requested 535. Please try again in 1.248s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "16it [01:11,  3.00s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 9b1e9983-95db-4c02-8792-46a9117754c8: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9629, Requested 396. Please try again in 150ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9629, Requested 396. Please try again in 150ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dd523fbc-d01a-4727-92b3-60374a78a27e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9618, Requested 407. Please try again in 150ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9618, Requested 407. Please try again in 150ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "17it [01:12,  2.15s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run e1790b35-ef47-49f3-822c-dda7f981db04: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9863, Requested 438. Please try again in 1.806s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9863, Requested 438. Please try again in 1.806s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "18it [01:15,  2.47s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 94c66ac6-365d-4ada-aac3-270122668698: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9888, Requested 420. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9888, Requested 420. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9b1e9983-95db-4c02-8792-46a9117754c8: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9942, Requested 496. Please try again in 2.628s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/llmops/lib/python3.12/site-packages/openai/_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-2d6U33yewajIVcwDdbidk9eI on tokens per min (TPM): Limit 10000, Used 9942, Requested 496. Please try again in 2.628s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "19it [01:17,  2.37s/it]Failed to batch ingest runs: LangSmithConnectionError(\"Connection error caused failure to POST https://api.smith.langchain.com/runs/batch  in LangSmith API. Please confirm your internet connection.. ConnectionError(ProtocolError('Connection aborted.', TimeoutError('The write operation timed out')))\")\n",
            "23it [01:31,  3.98s/it]\n"
          ]
        }
      ],
      "source": [
        "pdr_rag_results = evaluate(\n",
        "    ensemble_retriever_qa_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        unlabeled_coherence_evaluator,\n",
        "        labeled_relevance_evaluator\n",
        "        ],\n",
        "    experiment_prefix=\"Hybrid Retrieval RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPocfrNFiYWi"
      },
      "source": [
        "#### â“Question #1:\n",
        "\n",
        "What conclusions can you draw about the above results?\n",
        "\n",
        "Describe in your own words what the metrics are expressing.\n",
        "\n",
        "**Answer:** Due to rate limiting issues, not all metrics were evaluated. LangSmith turned out to be more robust than RAGAS in dealing with rate limiting issues and still making progress. Capturing datasets and mapping out experiments and results is much easier with Langsmith than RAGAS. \n",
        "\n",
        "The 3 evaluators that were reliably captured and plotted are `Coherence`, `COT_QA Contextual Accuracy` and `Dopeness`. On all the evaluations, the results were mixed with no clear winner. The Parent retriever excelled in Chain of Thought (COT), while Base did reasonably well in all evaluations. Hybrid seems to have done overall. \n",
        "\n",
        "![image](https://raw.githubusercontent.com/KPGAData/AIE3/main/Week%205/Evaluation%20across%20Retrievers.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.1.undefined"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "087e4c759a35424385156c76e3780144": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef2e833ac73548ee837f906d4b004a3d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2c984bd9861c4ec0b27f0c3a2020dede",
            "value": ""
          }
        },
        "13c75ca5ee9d42eb8476a307f30b7528": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "142e5756a6c840aeb25b733ece015ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4777a17d7d9c444a8d44e070faef0ad0",
              "IPY_MODEL_dad645f3259c41b5a50e0e94906ca3e3",
              "IPY_MODEL_c67428191d074e4d80cb0f28fb65cd41"
            ],
            "layout": "IPY_MODEL_8db8082c5ab741b5b4fd78027e65135d"
          }
        },
        "18ffc7ae91d1408c8747f8d0a972f3a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9927b2052be4016b0feb88ab583d32e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fbddf6663954ff48ee0cb0b4a202acb",
            "value": 1
          }
        },
        "1d0589c63fd1461b93b403c594b6ccfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27e2bff47a084ed6bc7d96ac09af9ebc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2ab8452345f24b46b172515ddc77fdb0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c984bd9861c4ec0b27f0c3a2020dede": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42bfd25ca7a14d629e4070b093a197d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d0589c63fd1461b93b403c594b6ccfa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5e500672dafc4529806e4e5f72fe97fa",
            "value": ""
          }
        },
        "46a7a454e9bb486588ac33156abddecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4777a17d7d9c444a8d44e070faef0ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ab8452345f24b46b172515ddc77fdb0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d5ad942ac5ed41c6a581b4022f177114",
            "value": ""
          }
        },
        "5e500672dafc4529806e4e5f72fe97fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65d1b263094a4c6abdaf8688285c41cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_087e4c759a35424385156c76e3780144",
              "IPY_MODEL_18ffc7ae91d1408c8747f8d0a972f3a8",
              "IPY_MODEL_8cbac09f7ba540bd8f79dde47e49fe5f"
            ],
            "layout": "IPY_MODEL_b5e2be01c1834c78b7ce0cbe1f7a9d09"
          }
        },
        "6fbddf6663954ff48ee0cb0b4a202acb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7448f6f5d9dc431fba689d8821285ca3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cbac09f7ba540bd8f79dde47e49fe5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6035d68dd9e4c6598ac5eae2d0deac1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b6b1f2be05a34e118dc55869149b60ac",
            "value": "â€‡12/?â€‡[01:07&lt;00:00,â€‡â€‡2.29s/it]"
          }
        },
        "8db8082c5ab741b5b4fd78027e65135d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a603cf6ec7d544a1a951fb8defccb976": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f79f87ac933b45fa9e2c17871d4f2e44",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a888bb92387549fea8843f20e3b4c50f",
            "value": "â€‡22/?â€‡[01:19&lt;00:00,â€‡â€‡2.61s/it]"
          }
        },
        "a888bb92387549fea8843f20e3b4c50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5e2be01c1834c78b7ce0cbe1f7a9d09": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6b1f2be05a34e118dc55869149b60ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c67428191d074e4d80cb0f28fb65cd41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13c75ca5ee9d42eb8476a307f30b7528",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e8143f8ceee6468c98433734ff59b835",
            "value": "â€‡22/?â€‡[01:16&lt;00:00,â€‡â€‡2.24s/it]"
          }
        },
        "c9927b2052be4016b0feb88ab583d32e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d097ee02256d40d5b395d37cd0face05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5ad942ac5ed41c6a581b4022f177114": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dad645f3259c41b5a50e0e94906ca3e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e2bff47a084ed6bc7d96ac09af9ebc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d097ee02256d40d5b395d37cd0face05",
            "value": 1
          }
        },
        "e6035d68dd9e4c6598ac5eae2d0deac1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8143f8ceee6468c98433734ff59b835": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e99b5317081e42c199a297f3a483bbf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42bfd25ca7a14d629e4070b093a197d5",
              "IPY_MODEL_fa9f9fc79b344271adc94acc487aef59",
              "IPY_MODEL_a603cf6ec7d544a1a951fb8defccb976"
            ],
            "layout": "IPY_MODEL_7448f6f5d9dc431fba689d8821285ca3"
          }
        },
        "ece363bdc8b54631beba2628e3175e0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ef2e833ac73548ee837f906d4b004a3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f79f87ac933b45fa9e2c17871d4f2e44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa9f9fc79b344271adc94acc487aef59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ece363bdc8b54631beba2628e3175e0b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46a7a454e9bb486588ac33156abddecc",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
